{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.convert_brat_annotations_to_json import load_jsonl, used_entities\n",
    "def span_match(span_1, span_2) :\n",
    "    sa, ea = span_1\n",
    "    sb, eb = span_2\n",
    "    iou = (min(ea, eb) - max(sa, sb)) / (max(eb, ea) - min(sa, sb))\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = used_entities + ['None']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data = load_jsonl('../model_data/all_data_propagated.jsonl')[:440]\n",
    "original_data = load_jsonl('../model_data/all_data_original_propagated.jsonl')[:440]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "for d in original_data :\n",
    "    span_to_cluster_ids = defaultdict(list)\n",
    "    for cluster_name in d['coref']:\n",
    "        for span in d['coref'][cluster_name]:\n",
    "            span_to_cluster_ids[tuple(span)].append(cluster_name)\n",
    "\n",
    "    span_to_cluster_ids = {span: set(sorted(v)) for span, v in span_to_cluster_ids.items()}\n",
    "    d['span_to_cluster'] = span_to_cluster_ids\n",
    "    \n",
    "for d in annotated_data :\n",
    "    span_to_cluster_ids = defaultdict(list)\n",
    "    for cluster_name in d['coref']:\n",
    "        for span in d['coref'][cluster_name]:\n",
    "            span_to_cluster_ids[tuple(span)].append(cluster_name)\n",
    "\n",
    "    span_to_cluster_ids = {span: set(sorted(v)) for span, v in span_to_cluster_ids.items()}\n",
    "    d['span_to_cluster'] = span_to_cluster_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def matching(spans_1, spans_2) :\n",
    "    match = np.zeros((len(spans_1), len(spans_2)))\n",
    "    match_types = {k + '_1':{l + '_2':0 for l in ents} for k in ents}\n",
    "    for i, s1 in enumerate(spans_1) :\n",
    "        for j, s2 in enumerate(spans_2) :\n",
    "            if (s1[0], s1[1]) == (s2[0], s2[1]) :\n",
    "                match[i, j] = 1\n",
    "                \n",
    "    assert all(x <= 1 for x in match.sum(0)), breakpoint()\n",
    "    assert all(x <= 1 for x in match.sum(1)), breakpoint()\n",
    "    for i, s1 in enumerate(spans_1) :\n",
    "        matched = [j for j, x in enumerate(match[i]) if x == 1]\n",
    "        assert len(matched) <= 1\n",
    "        if len(matched) == 0 :\n",
    "            match_types[s1[2] + '_1']['None_2'] += 1 / len(spans_1)\n",
    "        else :\n",
    "            match_types[s1[2] + '_1'][spans_2[matched[0]][2] + '_2'] += 1 \n",
    "            \n",
    "            \n",
    "    for i, s2 in enumerate(spans_2) :\n",
    "        matched = [j for j, x in enumerate(match[:, i]) if x == 1]\n",
    "        assert len(matched) <= 1\n",
    "        if len(matched) == 0 :\n",
    "            match_types['None_1'][s2[2] + '_2'] += 1 \n",
    "            \n",
    "    s = sum(y for k, v in match_types.items() for x, y in v.items())\n",
    "    for k, v in match_types.items() :\n",
    "        for x, y in v.items() :\n",
    "            match_types[k][x] /= s\n",
    "    return match_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 440/440 [00:44<00:00,  9.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "match_types = {k + '_1':{l + '_2':0 for l in ents} for k in ents}\n",
    "for i in tqdm(range(len(original_data))) :\n",
    "    match_types_doc = matching(original_data[i]['ner'], annotated_data[i]['ner'])\n",
    "    for k in match_types :\n",
    "        for l in match_types[k] :\n",
    "            match_types[k][l] += match_types_doc[k][l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "match_types = pd.DataFrame(match_types).loc[[e + '_2' for e in ents], [e + '_1' for e in ents]].T / len(annotated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Material_2    0.069475\n",
       "Metric_2      0.095343\n",
       "Task_2        0.210575\n",
       "Method_2      0.624067\n",
       "None_2        0.000540\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_types.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} &  Material &  Metric &  Task &  Method &  None \\\\\n",
      "\\midrule\n",
      "Material &      3.55 &    0.01 &  0.07 &    0.16 &  0.03 \\\\\n",
      "Metric   &      0.02 &    7.95 &  0.00 &    0.03 &  0.00 \\\\\n",
      "Task     &      0.32 &    0.07 & 17.92 &    0.44 &  0.01 \\\\\n",
      "Method   &      0.65 &    0.21 &  0.24 &   53.27 &  0.02 \\\\\n",
      "None     &      2.40 &    1.30 &  2.82 &    8.50 &  0.00 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(match_types.rename(index=lambda x : x.replace('_1', ''), columns=lambda x : x.replace('_2', '')).to_latex(float_format=lambda x : \"{:0.2f}\".format(x*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def matching_links(spans_1, spans_2, links_1, links_2) :\n",
    "    match = np.zeros((len(spans_1), len(spans_2)))\n",
    "    match_types = {k + '_2':{'Exist' : {'+' : 0, '-' : 0}, 'New' : {'+' : 0, '-' : 0}} for k in ents}\n",
    "    for i, s1 in enumerate(spans_1) :\n",
    "        for j, s2 in enumerate(spans_2) :\n",
    "            if span_match((s1[0], s1[1]), (s2[0], s2[1])) > 0.5 :\n",
    "                match[i, j] = 1\n",
    "                \n",
    "    assert all(x <= 1 for x in match.sum(0)), breakpoint()\n",
    "    assert all(x <= 1 for x in match.sum(1)), breakpoint()\n",
    "            \n",
    "            \n",
    "    for i, s2 in enumerate(spans_2) :\n",
    "        matched = [j for j, x in enumerate(match[:, i]) if x == 1]\n",
    "        assert len(matched) <= 1\n",
    "        is_matched = True if len(matched) > 0 and spans_1[matched[0]][2] == s2[2] else False\n",
    "        if not is_matched :\n",
    "            t = 'New'\n",
    "            new_links = links_2.get(tuple(s2[:2]), set())\n",
    "            match_types[s2[2] + '_2'][t]['+'] += 1 if len(new_links) > 0 else 0\n",
    "        else :\n",
    "            t = 'Exist'\n",
    "            matched_span = spans_1[matched[0]]\n",
    "            new_links, old_links = links_2.get(tuple(s2[:2]), set()), links_1.get(tuple(matched_span[:2]), set())\n",
    "            match_types[s2[2] + '_2'][t]['+'] += 1 if len(new_links - old_links) > 0 else 0\n",
    "            match_types[s2[2] + '_2'][t]['-'] += 1 if len(old_links - new_links) > 0 else 0\n",
    "            \n",
    "            \n",
    "    return match_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 440/440 [01:07<00:00,  7.27it/s]\n"
     ]
    }
   ],
   "source": [
    "match_types = {k + '_2':{(t, q) : 0 for t in ['Exist', 'New'] for q in ['+', '-']} for k in ents}\n",
    "\n",
    "for i in tqdm(range(len(annotated_data))) :\n",
    "    m = matching_links(original_data[i]['ner'], annotated_data[i]['ner'], original_data[i]['span_to_cluster'], annotated_data[i]['span_to_cluster'])\n",
    "    for k in m :\n",
    "        m[k] = {(s, x):n for s, v in m[k].items() for x, n in v.items()}\n",
    "        for (s, x), n in m[k].items() :\n",
    "            match_types[k][(s, x)] += n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.448232323232324"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(match_types).sum().sum()*100 / (440*360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{l}{Exist} & \\multicolumn{2}{l}{New} \\\\\n",
      "{} &     + &    - &   + &   - \\\\\n",
      "\\midrule\n",
      "Material &   1.2 &  0.8 & 6.8 & 0.0 \\\\\n",
      "Metric   &   0.4 &  1.2 & 2.8 & 0.0 \\\\\n",
      "Task     &   0.9 &  1.6 & 5.4 & 0.0 \\\\\n",
      "Method   &   1.7 & 14.0 & 8.0 & 0.0 \\\\\n",
      "None     &   0.0 &  0.0 & 0.0 & 0.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print((pd.DataFrame(match_types).T / len(annotated_data)).rename(index=lambda x : x.replace('_2', '')).to_latex(float_format=\"{:0.1f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthakj/miniconda3/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from dygie.data.dataset_readers.read_pwc_dataset import is_x_in_y\n",
    "does_overlap = lambda x, y: max(x[0], y[0]) < min(x[1], y[1])\n",
    "\n",
    "def map_to_section_and_sentence(data) :\n",
    "    mentions = data['ner']\n",
    "    sections = data['sections']\n",
    "    sentences = data['sentences']\n",
    "    \n",
    "    for e in mentions:\n",
    "        in_sentences = [i for i, s in enumerate(data[\"sentences\"]) if is_x_in_y(e, s)]\n",
    "        if len(in_sentences) > 1:\n",
    "            breakpoint()\n",
    "        if len(in_sentences) == 0:\n",
    "            in_sentences = [i for i, s in enumerate(data[\"sentences\"]) if does_overlap(e, s)]\n",
    "            assert sorted(in_sentences) == list(range(min(in_sentences), max(in_sentences) + 1)), breakpoint()\n",
    "            # breakpoint()\n",
    "            in_sentences = sorted(in_sentences)\n",
    "            data[\"sentences\"][in_sentences[0]][1] = data[\"sentences\"][in_sentences[-1]][1]\n",
    "            data[\"sentences\"] = [s for i, s in enumerate(data[\"sentences\"]) if i not in in_sentences[1:]]\n",
    "    mention_to_section = {}\n",
    "    mention_to_sentence = {}\n",
    "    for s, e, _ in mentions :\n",
    "        done = False\n",
    "        for s_1, e_1 in sections :\n",
    "            if is_x_in_y((s, e), (s_1, e_1)):\n",
    "                mention_to_section[(s, e)] = (s_1, e_1)\n",
    "                done = True\n",
    "        assert done\n",
    "        \n",
    "    for s, e, _ in mentions :\n",
    "        done = False\n",
    "        for s_1, e_1 in sentences :\n",
    "            if is_x_in_y((s, e), (s_1, e_1)):\n",
    "                mention_to_sentence[(s, e)] = (s_1, e_1)\n",
    "                done = True\n",
    "        assert done\n",
    "        \n",
    "    data['mention_to_section'] = mention_to_section\n",
    "    data['mention_to_sentence'] = mention_to_sentence\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 440/440 [00:15<00:00, 27.74it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for d in tqdm(annotated_data) :\n",
    "    map_to_section_and_sentence(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.entity_utils import used_entities\n",
    "from itertools import combinations\n",
    "def stats_for_relation(data) :\n",
    "    n_ary_relations = data['n_ary_relations']\n",
    "    corefs = data['coref']\n",
    "    stats = {}\n",
    "    for card in range(2, 5) :\n",
    "        for elist in combinations(used_entities, card) :\n",
    "            stats[tuple(elist)] = {'In section':0, 'Out section':0, 'In sentence':0, 'Out sentence':0}\n",
    "            for rel in n_ary_relations :\n",
    "                rel = [rel[k] for k in elist]\n",
    "                clusters = [corefs[x] for x in rel]\n",
    "#                 if any(len(c) == 0 for c in clusters) :\n",
    "#                     continue\n",
    "                sections = [set([data['mention_to_section'][tuple(span)] for span in cluster]) \n",
    "                            for cluster in clusters]\n",
    "                sections = set.intersection(*sections)\n",
    "                if len(sections) > 0 :\n",
    "                    stats[tuple(elist)]['In section'] += 1\n",
    "                else :\n",
    "                    stats[tuple(elist)]['Out section'] += 1\n",
    "#                     if card == 4 :\n",
    "#                         print(data['doc_id'], rel)\n",
    "                    \n",
    "                sections = [set([data['mention_to_sentence'][tuple(span)] for span in cluster]) \n",
    "                            for cluster in clusters]\n",
    "                sections = set.intersection(*sections)\n",
    "                if len(sections) > 0 :\n",
    "                    stats[tuple(elist)]['In sentence'] += 1\n",
    "                else :\n",
    "                    stats[tuple(elist)]['Out sentence'] += 1\n",
    "                    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439/439 [00:00<00:00, 1177.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "stats = stats_for_relation(annotated_data[0])\n",
    "for d in tqdm(annotated_data[1:]) :\n",
    "    stats_d = stats_for_relation(d)\n",
    "    for key in stats_d :\n",
    "        for v, x in stats_d[key].items() :\n",
    "            stats[key][v] += x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stats = {\"/\".join(k):v for k, v in stats.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(stats).T / 440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} &  In section &  In sentence &  Out section &  Out sentence \\\\\n",
      "n\\_ary &             &              &              &               \\\\\n",
      "\\midrule\n",
      "2     &      12.380 &        6.630 &       16.980 &        22.730 \\\\\n",
      "3     &       4.955 &        0.591 &       14.618 &        18.982 \\\\\n",
      "4     &       0.707 &        0.011 &        4.186 &         4.882 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "metrics['n_ary'] = pd.Series(metrics.index).apply(lambda x : len(x.split('/'))).values\n",
    "print(metrics.groupby('n_ary').agg(np.sum).to_latex(float_format=lambda x: \"{:0.3f}\".format(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9930423782416193"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.57 / (1.57 + 0.011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.36"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12.38+16.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.entity_utils import used_entities\n",
    "from itertools import combinations\n",
    "def stats_for_relation(data) :\n",
    "    n_ary_relations = data['n_ary_relations']\n",
    "    corefs = data['coref']\n",
    "    stats = {}\n",
    "    for card in range(2, 5) :\n",
    "        rels = []\n",
    "        for elist in combinations(used_entities, card) :\n",
    "            for rel in n_ary_relations :\n",
    "                rel = [rel[k] for k in elist]\n",
    "                rels.append(tuple(rel))\n",
    "                \n",
    "        rels = set(rels)\n",
    "        stats[card] = len(rels)\n",
    "                    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 439/439 [00:00<00:00, 13902.37it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "stats = stats_for_relation(annotated_data[0])\n",
    "for d in tqdm(annotated_data[1:]) :\n",
    "    stats_d = stats_for_relation(d)\n",
    "    for key, x in stats_d.items() :\n",
    "        stats[key] += x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.072727272727274"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7072/440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
